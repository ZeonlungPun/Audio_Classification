from WaveformLogmel import  CNNClassifier
from WaveformLogmelDatasets import AudioDataset,AudioDataset2
from torch.utils.data import WeightedRandomSampler
from collections import Counter, defaultdict
import os,torch
from torch.utils.data import  DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score,confusion_matrix, ConfusionMatrixDisplay
import torch.nn as nn

def inference(model, val_dl, class_names,device):
    model.eval()
    correct_prediction = 0
    total_prediction = 0
    all_preds = []
    all_labels = []
    class_correct = defaultdict(int)
    class_total = defaultdict(int)

    with torch.no_grad():

        for spec,signal, labels in tqdm(val_dl, desc="Evaluating", leave=False):
            spec, signal, labels = spec.to(device),signal.to(device) ,labels.to(device)
            # Forward

            outputs, _ = model(spec)

            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            # çµ±è¨ˆæ•´é«”æ­£ç¢ºç‡
            correct_prediction += (preds == labels).sum().item()
            total_prediction += preds.size(0)

            # çµ±è¨ˆåˆ†é¡æ­£ç¢ºç‡
            for label, pred in zip(labels, preds):
                class_total[label.item()] += 1
                if label == pred:
                    class_correct[label.item()] += 1

    # æ•´é«”æ­£ç¢ºç‡
    acc = correct_prediction / total_prediction
    macro_f1 = f1_score(all_labels, all_preds, average='macro')
    print(f'\nâœ… Overall Accuracy: {acc:.2f}, Macro-F1: {macro_f1:.2f}')
    print(f'Total items: {total_prediction}')
    cm = confusion_matrix(all_labels, all_preds)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
    disp.plot(cmap=plt.cm.Blues, xticks_rotation=45)
    plt.title(f'Confusion Matrix (Acc: {acc:.4f}, F1: {macro_f1:.4f})')
    plt.tight_layout()
    plt.savefig('cm.png')

    # æ¯å€‹é¡åˆ¥çš„æ­£ç¢ºç‡
    print('\nğŸ“Š Per-Class Accuracy:')
    for i, class_name in enumerate(class_names):
        total = class_total[i]
        correct = class_correct[i]
        acc = correct / total if total > 0 else 0.0
        print(f'  {class_name}: {acc:.2f} ({correct}/{total})')
    return macro_f1
# Training Loop
# ----------------------------
def training(model, train_dl, num_epochs, val_dl, class_names1,class_names2,save_path='best.pth',one_hot_label=False):
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    if one_hot_label:
        criterion =nn.BCEWithLogitsLoss()
    else:
        criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
    scheduler = torch.optim.lr_scheduler.OneCycleLR(
        optimizer, max_lr=0.001,
        steps_per_epoch=len(train_dl),
        epochs=num_epochs,
        anneal_strategy='linear'
    )
    best_macro_f1 = 0.0  # â† åˆå§‹åŒ–æœ€ä½³é©—è­‰æº–ç¢ºç‡

    # ğŸ” å¦‚æœæœ‰å·²ä¿å­˜çš„æ¨¡å‹ï¼Œè®€å–æ¨¡å‹åƒæ•¸ä¸¦è¼‰å…¥
    if os.path.exists(save_path):
        print(f"ğŸ“¥ Loading existing model from '{save_path}'...")
        model.load_state_dict(torch.load(save_path, map_location=device))
        model.to(device)
        # ä¸€æ¬¡é©—è­‰ï¼Œæ›´æ–° best_macro_f1ï¼š
        if val_dl is not None:
            best_macro_f1 = inference(model, val_dl, class_names1,device,using_hidden)
            print(f"ğŸ” Loaded model macro-F1: {best_macro_f1:.4f}")

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct_prediction = 0
        total_prediction = 0

        # ğŸ”„ ä½¿ç”¨ tqdm é¡¯ç¤ºæ¯å€‹ epoch çš„é€²åº¦
        loop = tqdm(train_dl, desc=f"Epoch [{epoch+1}/{num_epochs}]", leave=False)
        for spec,signal, labels in loop:

            spec,signal, labels = spec.to(device),signal.to(device) ,labels.to(device)
            model.to(device)

            optimizer.zero_grad()
            outputs = model(spec)


            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            scheduler.step()

            running_loss += loss.item()
            _, prediction = torch.max(outputs, 1)

            if one_hot_label:
                _,labels= torch.max(labels,1)

            correct_prediction += (prediction == labels).sum().item()
            total_prediction += prediction.shape[0]

            # ğŸŸ¦ æ›´æ–° tqdm é¡¯ç¤ºå…§å®¹
            loop.set_postfix(loss=loss.item(), acc=correct_prediction / total_prediction)

        # æ¯å€‹ epoch çµæŸå¾Œåˆ—å°çµæœ
        epoch_loss = running_loss / len(train_dl)
        epoch_acc = correct_prediction / total_prediction
        print(f"âœ… Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.4f}")

        # æ¯å¹¾å€‹ epoch å‘¼å«ä¸€æ¬¡é©—è­‰å‡½æ•¸
        if val_dl is not None and ( (epoch+1) %5==0 or epoch==num_epochs):
            macro_f1=inference(model, val_dl, class_names2,device,using_hidden)
            if macro_f1 > best_macro_f1:
                best_macro_f1 = macro_f1
                torch.save(model.state_dict(), save_path)
                print(f'ğŸ¯ Best model updated and saved (Macro-F1: {macro_f1:.4f})')


if __name__ == '__main__':

    train_path = '/home/zonekey/project/audio_classification/train'
    train_path2= '/home/zonekey/project/audio_classification/train'
    class_names_list1 = [ 'clap']
    class_names_list1_ = ['aloud', 'discuss', 'noise', 'single']
    val_path = '/home/zonekey/project/audio_classification/clean_dataset/val_manual'
    val_path2 = '/home/zonekey/project/audio_classification/clean_dataset/val_manual'
    class_names_list2 = ['clap']
    class_names_list2_ = ['aloud', 'discuss', 'noise', 'single']
    class_names_list = ['aloud','clap', 'discuss', 'noise', 'single']
    wave_path_list1 = []
    label_list1 = []
    wave_path_list2 = []
    label_list2 = []

    for label_idx, class_name in enumerate(class_names_list1):
        class_dir = os.path.join(train_path, class_name)
        for filename in os.listdir(class_dir):
            if filename.endswith(".wav"):
                wave_path_list1.append(os.path.join(class_dir, filename))
                label_list1.append(class_names_list.index(class_name))  # æ•¸å­—å½¢å¼çš„ label

    for label_idx, class_name in enumerate(class_names_list1_):
        class_dir = os.path.join(train_path2, class_name)
        for filename in os.listdir(class_dir):
            if filename.endswith(".wav"):
                wave_path_list1.append(os.path.join(class_dir, filename))
                label_list1.append(class_names_list.index(class_name))  # æ•¸å­—å½¢å¼çš„ label

    label_counter1 = Counter(label_list1)
    print("æ¯é¡æ¨£æœ¬æ•¸é‡:", label_counter1)

    # ç‚ºæ¯å€‹æ¨£æœ¬è¨­ç½®ç›¸å°æ‡‰çš„æ¬Šé‡ï¼ˆæ¨£æœ¬è¶Šå°‘æ¬Šé‡è¶Šé«˜ï¼‰
    sample_weights1 = [1.0 / label_counter1[label] for label in label_list1]

    for label_idx, class_name in enumerate(class_names_list2):
        class_dir = os.path.join(val_path, class_name)
        for filename in os.listdir(class_dir):
            if filename.endswith(".wav"):
                wave_path_list2.append(os.path.join(class_dir, filename))
                label_list2.append(class_names_list.index(class_name))  # æ•¸å­—å½¢å¼çš„ label

    for label_idx, class_name in enumerate(class_names_list2_):
        class_dir = os.path.join(val_path2, class_name)
        for filename in os.listdir(class_dir):
            if filename.endswith(".wav"):
                wave_path_list2.append(os.path.join(class_dir, filename))
                label_list2.append(class_names_list.index(class_name))  # æ•¸å­—å½¢å¼çš„ label

    label_counter2 = Counter(label_list2)
    print("æ¯é¡æ¨£æœ¬æ•¸é‡:", label_counter2)

    # ç‚ºæ¯å€‹æ¨£æœ¬è¨­ç½®ç›¸å°æ‡‰çš„æ¬Šé‡ï¼ˆæ¨£æœ¬è¶Šå°‘æ¬Šé‡è¶Šé«˜ï¼‰
    sample_weights2 = [1.0 / label_counter2[label] for label in label_list2]

    sampler1 = WeightedRandomSampler(
        weights=sample_weights1,
        num_samples=len(sample_weights1),
        replacement=True
    )

    train_dataset = AudioDataset2(wave_path_list=wave_path_list1, target_sample_rate=16000, duration=1000,
                           class_names_list=class_names_list,target_channel=1,augment=True)
    train_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=sampler1)
    sampler2 = WeightedRandomSampler(
        weights=sample_weights2,
        num_samples=len(sample_weights2),
        replacement=True
    )

    val_dataset = AudioDataset(wave_path_list=wave_path_list2, target_sample_rate=16000, duration=1000,
                           class_names_list=class_names_list,target_channel=1)
    val_loader = DataLoader(dataset=val_dataset, batch_size=16, sampler=sampler2)


    model = CNNClassifier(num_classes=5)
    num_epochs = 50
    training(model, train_loader, num_epochs, val_loader, class_names_list,class_names_list,save_path='pure_cnn512.pth',one_hot_label=True)











